{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "# word dict\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class WordCounter2Dict():\n",
    "    '''\n",
    "    vocab as keys, freq as value, dict\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.word_dict = None\n",
    "                 \n",
    "    def get_counter(self, counter, MAX_VOCAB_SIZE):\n",
    "        self.max = MAX_VOCAB_SIZE - 1 if MAX_VOCAB_SIZE is not \"all\" else None\n",
    "        self.word_dict = dict(counter.most_common(self.max))\n",
    "#         for not common vocab marked as '<unk>', freq = 0\n",
    "#         self.word_dict['<unk>'] = sum(list(counter.values())) - sum(list(self.word_dict.values()))\n",
    "        self.word_dict['<unk>'] = 0\n",
    "    def __str__(self):\n",
    "        return \"{}\".format(self.word_dict)\n",
    "#     id = enumerate in word word_dict.keys()\n",
    "    def getlen(self):\n",
    "        return len(self.word_dict)\n",
    "    def id2word(self):\n",
    "        return list(self.word_dict.keys())\n",
    "    def word2id(self):\n",
    "        return {word: wordid for wordid, word in enumerate(list(self.word_dict))}\n",
    "    def word_counts(self):\n",
    "        return np.array(list(self.word_dict.values()), dtype = np.float32)\n",
    "    def normalized(self):\n",
    "        return self.word_counts() / sum(self.word_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from scipy import stats\n",
    "\n",
    "class WordDataset(data.Dataset):\n",
    "    def __init__(self, word_list, word_dict, WINDOW, INTERVAL, NEGATIVE):\n",
    "        super().__init__()\n",
    "        self.word_list = word_list\n",
    "        self.word2id = word_dict.word2id()\n",
    "        self.word_counts = torch.from_numpy(word_dict.word_counts())\n",
    "        self.normalized = torch.from_numpy(word_dict.normalized())\n",
    "        self.word_weight = torch.from_numpy(np.array([1-weight for weight in self.word_counts]))\n",
    "#         zscore tfreq dict\n",
    "#         self.word_zscore_tfreq = torch.from_numpy(stats.zscore(self.word_counts))\n",
    "        \n",
    "#         word list by id according to all text but not sentences\n",
    "        self.encoded = [self.word2id.get(word, self.word2id['<unk>']) for word in self.word_list]\n",
    "        self.encoded = torch.Tensor(self.encoded).long()\n",
    "    \n",
    "        self.window = WINDOW\n",
    "        self.negative = NEGATIVE\n",
    "        self.interval = INTERVAL\n",
    "    def __len__(self):\n",
    "        return len(self.word_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        dataset magic function\n",
    "        \n",
    "        index: word index in word list\n",
    "        \n",
    "        return including:\n",
    "        central vocab\n",
    "        context vocabs\n",
    "        negative samples\n",
    "        '''   \n",
    "#         central vocab id\n",
    "        cen_vocab_id = self.encoded[index]\n",
    "\n",
    "# ------window types\n",
    "\n",
    "#         1. interval window\n",
    "#         former = [index - (t * self.interval) for t in range(1, self.window+1)]\n",
    "#         former.reverse()\n",
    "#         latter = [index + (t * self.interval) for t in range(1, self.window+1)]\n",
    "#         window_index = former + latter\n",
    "#         2. normal window\n",
    "        window_index = list(range(index - self.window, index)) + list(range(index + 1, index + self.window + 1))\n",
    "        index_con_vocab = [i % len(self.word_list) for i in window_index] # circle index\n",
    "        \n",
    "#         context vocabs id\n",
    "        con_vocabs_id = self.encoded[index_con_vocab]\n",
    "# ------weight\n",
    "        con_vocabs_weight = self.word_weight[con_vocabs_id]\n",
    "\n",
    "# ------sampling types\n",
    "# and in model.py\n",
    "        \n",
    "#         1. Negative sampling in (self.word_tfreq), for (NEGATIVE * con_vocabs_id.shape[0]) times\n",
    "#         (con_vocabs_id.shape): LIST of context vocabs， pick up self.negative * con_vocabs_id.shape[0] words\n",
    "        neg_samples_id = torch.multinomial(self.normalized, self.negative * con_vocabs_id.shape[0], True)       \n",
    "#         2. no negative sampling\n",
    "#         neg_samples_id = 0\n",
    "    \n",
    "        return cen_vocab_id, con_vocabs_id, con_vocabs_weight, neg_samples_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) torch.Size([32, 6]) torch.Size([32, 6]) torch.Size([32, 120])\n"
     ]
    }
   ],
   "source": [
    "# MAX_VOCAB_SIZE = \"all\" # the vocabulary size 词汇表多大\n",
    "# WINDOW = 3 # context window\n",
    "# INTERVAL = 2\n",
    "# NEGATIVE = 20 # number of negative samples\n",
    "# BATCH_SIZE = 32\n",
    "# # test\n",
    "\n",
    "# word_dict = WordCounter2Dict()\n",
    "# word_list = []\n",
    "# with open(\"source/test.txt\", 'r', encoding='utf-8') as fin:\n",
    "#     c = Counter()\n",
    "#     for line in fin.readlines():\n",
    "#         text = line.strip().split(\" \") #分割成词列表\n",
    "#         word_list.extend(text)\n",
    "#         c += Counter(text)\n",
    "#     word_dict.get_counter(c, MAX_VOCAB_SIZE)\n",
    "    \n",
    "# word_dataset = WordDataset(word_list, word_dict, WINDOW, INTERVAL, NEGATIVE)\n",
    "# dataloader = data.DataLoader(word_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# for i, (cen_vocab_id, con_vocabs, con_vocabs_weight, neg_samples) in enumerate(dataloader):\n",
    "#     if i == 0:\n",
    "#         print(cen_vocab_id.size(), con_vocabs.size(), con_vocabs_weight.size(), neg_samples.size())\n",
    "#         break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "= torch.Size([32]) torch.Size([32, 1]) torch.Size([32, 6]) torch.Size([32, 600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
